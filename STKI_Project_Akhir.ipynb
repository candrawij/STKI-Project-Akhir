{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYm2gE73arRp",
        "outputId": "9dbf7681-0621-4eb3-87bc-54244b76f99c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in c:\\users\\candra\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\Candra\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install Sastrawi\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc_ID         Nama_Tempat                      Lokasi  Rating  \\\n",
            "0       1  Kuncen Camp Ground  Kab. Semarang, Jawa Tengah     5.0   \n",
            "1       2  Kuncen Camp Ground  Kab. Semarang, Jawa Tengah     5.0   \n",
            "2       3  Kuncen Camp Ground  Kab. Semarang, Jawa Tengah     5.0   \n",
            "3       4  Kuncen Camp Ground  Kab. Semarang, Jawa Tengah     5.0   \n",
            "4       5  Kuncen Camp Ground  Kab. Semarang, Jawa Tengah     5.0   \n",
            "\n",
            "                                         Teks_Mentah  \n",
            "0  Bagus banget tempatnya, terkonsep dan guide ny...  \n",
            "1  Sangat menyenangkan untuk camping ceria.\\r\\nNy...  \n",
            "2  Tempatnya asri dan sejuk, sudah lumayan ramai ...  \n",
            "3  3 kali ke sini, sekali ikut acara, 2 kali biki...  \n",
            "4  Tempat yang cocok untuk acara kemah, kami kema...  \n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "df_corpus = pd.read_csv('Documents\\corpus_kemah_jateng_diy - Sheet4.csv')\n",
        "\n",
        "print(df_corpus.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2jOUj7bsdfRh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'tidak' dihapus dari stopwords untuk menangani negasi.\n",
            "'kurang' dihapus dari stopwords untuk menangani negasi.\n",
            "'jangan' dihapus dari stopwords untuk menangani negasi.\n",
            "'bukan' dihapus dari stopwords untuk menangani negasi.\n",
            "'tanpa' dihapus dari stopwords untuk menangani negasi.\n"
          ]
        }
      ],
      "source": [
        "# Import Sastrawi\n",
        "try:\n",
        "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "except ImportError:\n",
        "    print(\"Warning: Sastrawi is not available. Using Dummy Stemmer.\")\n",
        "    class DummyStemmer:\n",
        "        def stem(self, text):\n",
        "            return text\n",
        "    class StemmerFactory:\n",
        "        def create_stemmer(self):\n",
        "            return DummyStemmer()\n",
        "\n",
        "# Safety for NLTK Stopwords      \n",
        "try:\n",
        "    from nltk.corpus import stopwords\n",
        "    stopwords_id = set(stopwords.words('indonesian')) \n",
        "except (LookupError, ImportError):\n",
        "    print(\"Warning: Indonesian stopwords failed to load. Using minimal manual list.\")\n",
        "    stopwords_id = {\"yang\", \"dan\", \"di\", \"ke\", \"adalah\", \"dengan\", \"saya\", \"ini\"}\n",
        "\n",
        "negation_words = ['tidak', 'kurang', 'jangan', 'bukan', 'tanpa']\n",
        "for word in negation_words:\n",
        "    if word in stopwords_id:\n",
        "        stopwords_id.remove(word)\n",
        "        print(f\"'{word}' dihapus dari stopwords untuk menangani negasi.\")\n",
        "\n",
        "# Inisialisasi Tools\n",
        "stemmer = StemmerFactory().create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_map_from_csv(filepath):\n",
        "    \"\"\"\n",
        "    Memuat file CSV (kolom A: key, kolom B: value) ke dalam dictionary.\n",
        "    Fungsi ini mengabaikan baris yang diawali '#' (untuk komentar)\n",
        "    dan mengabaikan kolom ekstra (seperti kolom 'kategori').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 'comment=#' memberi tahu pandas untuk mengabaikan baris yang diawali #\n",
        "        df = pd.read_csv(filepath, comment='#')\n",
        "        \n",
        "        # Ambil nama kolom pertama (A) dan kedua (B)\n",
        "        key_col = df.columns[0]\n",
        "        value_col = df.columns[1]\n",
        "        \n",
        "        # Hapus baris yang mungkin kosong di kolom A atau B\n",
        "        df = df.dropna(subset=[key_col, value_col])\n",
        "        \n",
        "        # Buat dictionary: Kolom A jadi key, Kolom B jadi value\n",
        "        mapper_dict = pd.Series(df[value_col].values, index=df[key_col]).to_dict()\n",
        "        \n",
        "        print(f\"Berhasil memuat {len(mapper_dict)} aturan dari {filepath}\")\n",
        "        return mapper_dict\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"!!! PERINGATAN: File konfigurasi {filepath} tidak ditemukan. Menggunakan dictionary kosong.\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR saat memuat {filepath}: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat 32 aturan dari Kamus\\config_phrase_map - Sheet1.csv\n"
          ]
        }
      ],
      "source": [
        "# Definisi Pemegangan Frasa Kompleks\n",
        "def substitute_complex_phrases(text, phrase_map):\n",
        "    \"\"\"Mengganti frasa kompleks dengan single token sebelum tokenisasi.\"\"\"\n",
        "    # Pastikan substitusi dilakukan pada teks lowercase\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    for phrase, token in phrase_map.items():\n",
        "        # Lakukan penggantian frasa dalam teks\n",
        "        # Misalnya: \"dataran tinggi\" diganti menjadi \"datarantinggi\"\n",
        "        text_lower = text_lower.replace(phrase, token)\n",
        "        \n",
        "    return text_lower\n",
        "\n",
        "# Muat PHRASE_MAP dari file eksternal\n",
        "PHRASE_MAP = load_map_from_csv('Kamus\\config_phrase_map - Sheet1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat 23 aturan dari Kamus\\config_region_map - Sheet1.csv\n"
          ]
        }
      ],
      "source": [
        "# Muat REGION_MAP dari file eksternal\n",
        "REGION_MAP = load_map_from_csv('Kamus\\config_region_map - Sheet1.csv')\n",
        "\n",
        "def detect_region_and_filter_query(query_text):\n",
        "    \"\"\"\n",
        "    Menganalisis query untuk menentukan apakah mengandung niat regional.\n",
        "    Mengembalikan query yang sudah difilter (tanpa kata regional) dan kode region.\n",
        "    \"\"\"\n",
        "    \n",
        "    query_text_lower = query_text.lower()\n",
        "    detected_region = None\n",
        "    \n",
        "    # Deteksi Region\n",
        "    for term, region in REGION_MAP.items():\n",
        "        if term in query_text_lower:\n",
        "            detected_region = region\n",
        "            query_text_lower = query_text_lower.replace(term, '') # Hapus kata regional\n",
        "            # Break setelah region pertama terdeteksi (asumsi hanya 1 region per query)\n",
        "            break \n",
        "            \n",
        "    # Rebuild Query tanpa kata regional (untuk VSM)\n",
        "    # Hapus spasi berlebihan dan filter token kosong\n",
        "    filtered_query_text = \" \".join([word for word in query_text_lower.split() if word])\n",
        "    \n",
        "    return filtered_query_text, detected_region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat 12 aturan dari Kamus\\config_special_intent - Sheet1.csv\n"
          ]
        }
      ],
      "source": [
        "# Muat SPECIAL_INTENT_MAP dari file eksternal\n",
        "SPECIAL_INTENT_MAP = load_map_from_csv('Kamus\\config_special_intent - Sheet1.csv')\n",
        "\n",
        "def detect_intent(query_text):\n",
        "    \"\"\"\n",
        "    Menganalisis query untuk menentukan niat khusus (ALL/RATING).\n",
        "    Mengembalikan query VSM yang sudah bersih, dan special_intent.\n",
        "    \"\"\"\n",
        "    query_text_lower = query_text.lower()\n",
        "    special_intent = None\n",
        "    \n",
        "    # 1. Deteksi Niat Khusus (ALL/RATING)\n",
        "    for term, intent in SPECIAL_INTENT_MAP.items():\n",
        "        if term in query_text_lower:\n",
        "            special_intent = intent\n",
        "            query_text_lower = query_text_lower.replace(term, '')\n",
        "            break\n",
        "            \n",
        "    # 3. Rebuild Query VSM\n",
        "    filtered_query_text = \" \".join([word for word in query_text_lower.split() if word])\n",
        "    \n",
        "    return filtered_query_text, special_intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_full_query(query_text):\n",
        "    \"\"\"\n",
        "    Menganalisis query untuk intent, region, dan teks VSM terakhir.\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Deteksi Intent\n",
        "    # Ini membersihkan query dari frasa intent, misal \"tempat kemah terbaik\"\n",
        "    query_after_intent, special_intent = detect_intent(query_text)\n",
        "    \n",
        "    # 2. Deteksi Region\n",
        "    # Ini membersihkan query dari frasa region, misal \"jawa tengah\"\n",
        "    final_vsm_text, region_filter = detect_region_and_filter_query(query_after_intent)\n",
        "    \n",
        "    # 3. Preprocessing Teks VSM\n",
        "    vsm_tokens = full_preprocessing(final_vsm_text)\n",
        "\n",
        "    # Tambahan pengecekan khusus jika ada filter region\n",
        "    if region_filter:\n",
        "        # Gunakan kata yang sudah di-stem\n",
        "        generic_fluff_words = {'cari', 'tampil', 'lihat', 'berikan', 'saran', 'rekomendasikan'} \n",
        "        \n",
        "        # Cek apakah vsm_tokens HANYA berisi kata-kata fluff\n",
        "        if vsm_tokens and all(token in generic_fluff_words for token in vsm_tokens):\n",
        "            vsm_tokens = [] # Kosongkan token, jangan cari VSM\n",
        "    \n",
        "    # Jika token kosong setelah semua filter (misal query-nya hanya \"terbaik di jogja\")\n",
        "    if not vsm_tokens and (special_intent or region_filter):\n",
        "         # Beri kata kunci default agar VSM tidak error\n",
        "        vsm_tokens = ['kemah'] \n",
        "\n",
        "        # Jika intent awalnya kosong TAPI region ada,\n",
        "        # berarti pengguna HANYA ingin filter region. Ubah intent ke 'ALL'.\n",
        "        if not special_intent and region_filter:\n",
        "            special_intent = 'ALL'\n",
        "        \n",
        "    return vsm_tokens, special_intent, region_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. DEFENISI FUNGSI HELPER & VSM CLASSES ---\n",
        "# Fungsi Pembersihan Karakter Spesial\n",
        "def remove_special_characters(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" \n",
        "    regex = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "    return re.sub(regex, '', text)\n",
        "\n",
        "# Fungsi Proses Penuh (Preprocessing)\n",
        "def full_preprocessing(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "        \n",
        "    cleaned_text = remove_special_characters(text)\n",
        "    cleaned_text = re.sub(r'\\d', '', cleaned_text)\n",
        "\n",
        "    text_with_phrases = substitute_complex_phrases(cleaned_text, PHRASE_MAP)\n",
        "    \n",
        "    # Simple Tokenization (split by whitespace) & Lowercasing\n",
        "    words = text_with_phrases.lower().split()\n",
        "    \n",
        "    words = [w for w in words if w not in stopwords_id]\n",
        "    \n",
        "    # Stemming\n",
        "    stemmed_words = [stemmer.stem(w) for w in words]\n",
        "    \n",
        "    final_words = [w for w in stemmed_words if len(w) > 1]\n",
        "    return final_words\n",
        "\n",
        "# Inverted Index Classes\n",
        "class Node:\n",
        "    def __init__(self, docId, freq=None):\n",
        "        self.freq = freq # TF-IDF weight\n",
        "        self.doc = docId\n",
        "        self.nextval = None\n",
        "\n",
        "class SlinkedList:\n",
        "    def __init__(self, head=None):\n",
        "        self.head = head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. APLIKASI PREPROCESSING & HITUNG DF & IDF (INDEXING PHASE 1) ---\n",
        "# Pastikan dataset sudah dimuat di df_corpus\n",
        "df_corpus['Teks_Mentah'] = df_corpus['Teks_Mentah'].fillna('')\n",
        "df_corpus['Clean_Tokens'] = df_corpus['Teks_Mentah'].apply(full_preprocessing)\n",
        "\n",
        "N = len(df_corpus)\n",
        "df_counts = {} # Document Frequency\n",
        "\n",
        "for tokens in df_corpus['Clean_Tokens']:\n",
        "    for word in set(tokens): \n",
        "        df_counts[word] = df_counts.get(word, 0) + 1\n",
        "\n",
        "idf_scores = {}\n",
        "for term, count in df_counts.items():\n",
        "    idf_scores[term] = math.log10(N / count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. BUILDING THE INVERTED INDEX WITH TF-IDF (INDEXING PHASE 2) ---\n",
        "linked_list_data = {}\n",
        "unique_words_all = set(df_counts.keys())\n",
        "\n",
        "for word in unique_words_all:\n",
        "    linked_list_data[word] = SlinkedList()\n",
        "    linked_list_data[word].head = Node(docId=0, freq=None) \n",
        "\n",
        "for index, row in df_corpus.iterrows():\n",
        "    doc_id = row['Doc_ID']\n",
        "    tokens = row['Clean_Tokens']\n",
        "    \n",
        "    tf_in_doc = {}\n",
        "    for word in tokens:\n",
        "        tf_in_doc[word] = tf_in_doc.get(word, 0) + 1\n",
        "\n",
        "    for term, tf in tf_in_doc.items():\n",
        "        tfidf = tf * idf_scores[term]\n",
        "        \n",
        "        linked_list = linked_list_data[term].head\n",
        "        while linked_list.nextval is not None:\n",
        "            linked_list = linked_list.nextval\n",
        "        \n",
        "        linked_list.nextval = Node(docId=doc_id, freq=tfidf)\n",
        "\n",
        "# Mapping Doc ID to Name and Rating for final result\n",
        "df_metadata = df_corpus[['Doc_ID', 'Nama_Tempat', 'Lokasi', 'Rating']].copy()\n",
        "avg_rating_per_place = df_metadata.groupby('Nama_Tempat')['Rating'].mean().reset_index()\n",
        "avg_rating_per_place.rename(columns={'Rating': 'Avg_Rating'}, inplace=True)\n",
        "df_metadata = df_metadata.merge(avg_rating_per_place, on='Nama_Tempat', how='left')\n",
        "df_metadata.set_index('Doc_ID', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. FUNGSI VSM RANKING MURNI ---\n",
        "def search_by_keyword(query_tokens, special_intent, region_filter):\n",
        "    \"\"\"\n",
        "    Melakukan pencarian berdasarkan token VSM, intent, dan filter region.\n",
        "    \"\"\"\n",
        "    # Tangani kasus special_intent 'ALL'\n",
        "    if special_intent == 'ALL':\n",
        "        \n",
        "        # 1. Ambil semua tempat unik langsung dari metadata\n",
        "        df_unique_places = df_metadata[['Nama_Tempat', 'Lokasi', 'Avg_Rating']].drop_duplicates(subset='Nama_Tempat').copy()\n",
        "        \n",
        "        # 2. Terapkan filter regional jika ada\n",
        "        if region_filter:\n",
        "            # Menggunakan .str.contains() untuk mencocokkan substring (misal: 'diy' atau 'semarang')\n",
        "            df_unique_places = df_unique_places[df_unique_places['Lokasi'].str.lower().str.contains(region_filter, na=False)]\n",
        "\n",
        "        # 3. Urutkan berdasarkan Rating Tertinggi (sebagai default untuk 'ALL')\n",
        "        df_unique_places = df_unique_places.sort_values(by='Avg_Rating', ascending=False)\n",
        "        \n",
        "        # 4. Ubah format ke dictionary standar\n",
        "        final_recommendations = []\n",
        "        for _, row in df_unique_places.iterrows():\n",
        "            final_recommendations.append({\n",
        "                'name': row['Nama_Tempat'],\n",
        "                'location': row['Lokasi'],\n",
        "                'avg_rating': row['Avg_Rating'],\n",
        "                'top_vsm_score': 0.0  # Skor VSM 0.0 karena VSM tidak digunakan\n",
        "            })\n",
        "        \n",
        "        # Langsung kembalikan hasilnya\n",
        "        return final_recommendations\n",
        "\n",
        "    # 1. Preprocessing Query\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "    \n",
        "    # 2. Query Vectorization (TF-IDF)\n",
        "    query_tf = {}\n",
        "    for word in query_tokens:\n",
        "        query_tf[word] = query_tf.get(word, 0) + 1\n",
        "        \n",
        "    query_weights = {}\n",
        "    involved_docs = set()\n",
        "    \n",
        "    for term, tf in query_tf.items():\n",
        "        if term in idf_scores:\n",
        "            query_weights[term] = tf * idf_scores[term]\n",
        "            \n",
        "            # Collect all documents involved from the Index\n",
        "            current_node = linked_list_data[term].head.nextval\n",
        "            while current_node is not None:\n",
        "                involved_docs.add(current_node.doc)\n",
        "                current_node = current_node.nextval\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    if not involved_docs:\n",
        "        return []\n",
        "\n",
        "    # 3. Cosine Similarity (Dot Product Only)\n",
        "    doc_scores = {doc_id: 0 for doc_id in involved_docs}\n",
        "    \n",
        "    # Calculate DOT PRODUCT: Sum(W(t,d) * W(t,q))\n",
        "    for term, W_q in query_weights.items():\n",
        "        current_node = linked_list_data[term].head.nextval\n",
        "        while current_node is not None:\n",
        "            doc_id = current_node.doc\n",
        "            W_d = current_node.freq # TF-IDF weight W(t,d)\n",
        "            doc_scores[doc_id] += W_d * W_q\n",
        "            current_node = current_node.nextval\n",
        "            \n",
        "    # 4. Ranking Ulasan (Doc ID)\n",
        "    ranked_results_by_doc = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    \n",
        "    # 5. Agregasi ke Nama Tempat (Mengambil ulasan paling relevan per tempat)\n",
        "    final_recommendations = []\n",
        "    unique_names = set()\n",
        "    \n",
        "    for doc_id, vsm_score in ranked_results_by_doc:\n",
        "        try:\n",
        "            meta = df_metadata.loc[doc_id]\n",
        "        except KeyError:\n",
        "            continue\n",
        "        \n",
        "        # Filter berdasarkan region jika diminta (opsional)\n",
        "        if region_filter:\n",
        "            if region_filter not in meta['Lokasi'].lower():\n",
        "                continue # Skip dokumen yang tidak sesuai region\n",
        "            \n",
        "        name = meta['Nama_Tempat']\n",
        "        \n",
        "        if name not in unique_names:\n",
        "            unique_names.add(name)\n",
        "            final_recommendations.append({\n",
        "                'name': name,\n",
        "                'location': meta['Lokasi'],\n",
        "                'avg_rating': meta['Avg_Rating'],\n",
        "                'top_vsm_score': vsm_score\n",
        "            })\n",
        "\n",
        "    # 6. Logika Intent\n",
        "    # Terapkan sorting berdasarkan intent setelah VSM selesai\n",
        "    if special_intent == 'RATING_TOP':\n",
        "        # Urutkan berdasarkan Avg_Rating (Tertinggi ke Terendah)\n",
        "        final_recommendations.sort(key=lambda x: x['avg_rating'], reverse=True)\n",
        "    \n",
        "    elif special_intent == 'RATING_BOTTOM':\n",
        "        # Urutkan berdasarkan Avg_Rating (Terendah ke Tertinggi)\n",
        "        final_recommendations.sort(key=lambda x: x['avg_rating'], reverse=False)\n",
        "            \n",
        "    return final_recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saCnwIJzdgGw",
        "outputId": "70cb9409-de33-4e15-ca2e-abb73596afbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mesin pencarian siap untuk query selanjutnya...\n",
            "\n",
            "--------------------------------------------------\n",
            "HASIL PENCARIAN untuk: 'dekat telaga'\n",
            "Kata Kunci Diproses: ['telaga']\n",
            "Status Filter: No Region Filter Applied\n",
            "Mode Pencarian: Intent: VSM Relevancy\n",
            "--------------------------------------------------\n",
            "Rekomendasi Tempat Kemah (Diurutkan berdasarkan Relevansi Ulasan):\n",
            "1. Telaga Cebong\n",
            "   | Lokasi: Wonosobo, Jawa Tengah\n",
            "   | Rata-rata Rating Tempat: 4.71\n",
            "   | Skor Relevansi (VSM Score): 5.3031\n",
            "2. Telaga Dringo\n",
            "   | Lokasi: Banjarnegara, Jawa Tengah\n",
            "   | Rata-rata Rating Tempat: 4.40\n",
            "   | Skor Relevansi (VSM Score): 5.3031\n",
            "3. Camp Ground Bukit Sikunir\n",
            "   | Lokasi: Wonosobo, Jawa Tengah\n",
            "   | Rata-rata Rating Tempat: 4.60\n",
            "   | Skor Relevansi (VSM Score): 1.7677\n",
            "\n",
            "Sesi pencarian diakhiri. Terima kasih!\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================================\")\n",
        "print(\"MESIN PENCARIAN REKOMENDASI TEMPAT KEMAH VSM SIAP!\")\n",
        "print(\"==================================================\")\n",
        "print(\"Anda dapat memasukkan kata kunci untuk mencari rekomendasi.\")\n",
        "\n",
        "while True:\n",
        "    try:    \n",
        "        # Mengambil input query dari pengguna\n",
        "        query_text = input(\"\\nMasukkan kata kunci pencarian (atau ketik 'keluar' untuk berhenti): \\n\").strip()\n",
        "        \n",
        "        if query_text.lower() in ('keluar', 'exit', 'berhenti', 'quit', 'stop', 'kembali'):\n",
        "            print(\"\\nSesi pencarian diakhiri. Terima kasih!\")\n",
        "            break\n",
        "        \n",
        "        if not query_text:\n",
        "            continue\n",
        "\n",
        "        start_time = time.time()\n",
        "            \n",
        "        # Panggil fungsi pencarian VSM\n",
        "        vsm_tokens, intent, region = analyze_full_query(query_text)\n",
        "        \n",
        "        # 2. Panggil fungsi pencarian dengan hasil analisis\n",
        "        vsm_ranking = search_by_keyword(vsm_tokens, intent, region)\n",
        "\n",
        "        # Deteksi region untuk ditampilkan\n",
        "        filter_status = f\"Filtered by: {region.upper()}\" if region else \"No Region Filter Applied\"\n",
        "        intent_status = f\"Intent: {intent}\" if intent else \"Intent: VSM Relevancy\"\n",
        "        \n",
        "        print(\"\\n--------------------------------------------------\")\n",
        "        print(f\"HASIL PENCARIAN untuk: '{query_text}'\")\n",
        "        print(f\"Kata Kunci Diproses: {full_preprocessing(query_text)}\")\n",
        "        print(f\"Status Filter: {filter_status}\")\n",
        "        print(f\"Mode Pencarian: {intent_status}\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        if vsm_ranking:\n",
        "            if intent == 'RATING_TOP':\n",
        "                print(\"Rekomendasi Tempat Kemah (Diurutkan berdasarkan Rating Tertinggi):\")\n",
        "            elif intent == 'RATING_BOTTOM':\n",
        "                print(\"Rekomendasi Tempat Kemah (Diurutkan berdasarkan Rating Terendah):\")\n",
        "            else:\n",
        "                print(\"Rekomendasi Tempat Kemah (Diurutkan berdasarkan Relevansi Ulasan):\")\n",
        "            \n",
        "            for i, item in enumerate(vsm_ranking):\n",
        "                print(f\"{i+1}. {item['name']}\")\n",
        "                print(f\"   | Lokasi: {item['location']}\")\n",
        "                print(f\"   | Rata-rata Rating Tempat: {item['avg_rating']:.2f}\")\n",
        "                print(f\"   | Skor Relevansi (VSM Score): {item['top_vsm_score']:.4f}\")\n",
        "            \n",
        "            # Logika lanjut\n",
        "            continue_input = input(\"\\nApakah Anda ingin melanjutkan pencarian? (ya/tidak): \").strip().lower()\n",
        "            \n",
        "            if continue_input not in ('ya', 'y'):\n",
        "                print(\"\\nSesi pencarian diakhiri. Terima kasih!\")\n",
        "                break\n",
        "                \n",
        "            # Hapus output sebelum loop selanjutnya\n",
        "            clear_output(wait=True) \n",
        "            print(\"Mesin pencarian siap untuk query selanjutnya...\")\n",
        "        else:\n",
        "            print(\"Tidak ditemukan tempat kemah yang relevan dengan kata kunci ini.\")\n",
        "            \n",
        "            # Logika lanjut\n",
        "            continue_input = input(\"\\nApakah Anda ingin melanjutkan pencarian? (ya/tidak): \").strip().lower()\n",
        "            \n",
        "            if continue_input not in ('ya', 'y'):\n",
        "                print(\"\\nSesi pencarian diakhiri. Terima kasih!\")\n",
        "                break\n",
        "                \n",
        "            # Hapus output sebelum loop selanjutnya\n",
        "            clear_output(wait=True) \n",
        "            print(\"Mesin pencarian siap untuk query selanjutnya...\")\n",
        "\n",
        "    except KeyboardInterrupt:   \n",
        "        print(\"\\nSesi pencarian diakhiri oleh pengguna. Terima kasih!\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
